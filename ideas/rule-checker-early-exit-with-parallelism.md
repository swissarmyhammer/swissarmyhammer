# Rule Checker: Early Exit with Parallel Execution

**THIS IS A SCRATCH FILE GENERATED BY A ü§ñ**

## Date
2025-11-13

## The Problem

With parallel execution and `max_errors`, how do we **stop launching new work** when we've found enough violations?

### Example Scenario
- 1000 work items
- `concurrency = 4`
- `max_errors = 1` (fail-fast)

**Naive approach with buffer_unordered:**
```rust
stream::iter(work_queue)
    .buffer_unordered(4)  // Launches 4 futures
    .take(1)              // Stop after 1 result
```

**What happens:**
1. Launches 4 futures immediately
2. Future 1 completes with ERROR ‚Üí stream yields it
3. `.take(1)` stops consuming
4. **But futures 2, 3, 4 are still running!** (wasted LLM calls)
5. Stream drops, buffered futures complete but results are discarded

**Waste:** Up to `concurrency - 1` unnecessary LLM calls after we have max_errors.

## Solution Options

### Option 1: Accept the Waste (Simplest)

With `concurrency = 4`, worst case is 3 wasted LLM calls.

**Tradeoff:**
- ‚úÖ Simple implementation
- ‚úÖ Low waste with small concurrency
- ‚ùå Wasteful with large concurrency
- ‚ùå Rate limit consumption for no benefit

**Verdict:** Acceptable if concurrency ‚â§ 10

### Option 2: Cancellation Token (Best)

Use a shared atomic flag to signal "stop launching work":

```rust
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, Ordering};

let should_stop = Arc::new(AtomicBool::new(false));
let errors_found = Arc::new(AtomicUsize::new(0));

let stream = stream::iter(work_queue)
    .take_while(|_| {
        // Stop accepting new work if we've hit max_errors
        if let Some(max) = max_errors {
            errors_found.load(Ordering::Relaxed) < max
        } else {
            true
        }
    })
    .map(move |(rule, target)| {
        let checker = Arc::clone(&checker);
        let errors = Arc::clone(&errors_found);
        let stop = Arc::clone(&should_stop);

        async move {
            // Check if we should skip (another task found max_errors)
            if stop.load(Ordering::Relaxed) {
                return Ok(None);  // Skip this check
            }

            let result = checker.check_file(&rule, &target).await;

            // If we found an error, increment counter
            if let Ok(Some(violation)) = &result {
                if violation.severity == Severity::Error {
                    let count = errors.fetch_add(1, Ordering::Relaxed) + 1;
                    if let Some(max) = max_errors {
                        if count >= max {
                            stop.store(true, Ordering::Relaxed);
                        }
                    }
                }
            }

            result
        }
    })
    .buffer_unordered(concurrency)
    .filter_map(|result| async move {
        match result {
            Ok(Some(violation)) if violation.severity == Severity::Error => Some(Ok(violation)),
            Ok(Some(_)) => None,
            Ok(None) => None,
            Err(e) => Some(Err(e)),
        }
    });
```

**Benefits:**
- ‚úÖ Stops launching new work immediately
- ‚úÖ In-flight checks detect the flag and can short-circuit
- ‚úÖ Minimal waste (only in-flight work completes)

**Complexity:** Medium (~40 lines vs ~20)

### Option 3: JoinSet with Cancellation (Most Control)

Use `tokio::task::JoinSet` for full control:

```rust
use tokio::task::JoinSet;

let mut join_set = JoinSet::new();
let mut work_iter = work_queue.into_iter();
let mut violations = Vec::new();

// Fill initial batch
for _ in 0..concurrency.min(work_queue.len()) {
    if let Some((rule, target)) = work_iter.next() {
        let checker = Arc::clone(&checker);
        join_set.spawn(async move {
            checker.check_file(&rule, &target).await
        });
    }
}

// Process results and launch new work
while let Some(result) = join_set.join_next().await {
    match result {
        Ok(Ok(Some(violation))) if violation.severity == Severity::Error => {
            violations.push(violation);

            // Check if we hit max_errors
            if let Some(max) = max_errors {
                if violations.len() >= max {
                    join_set.shutdown().await;  // Cancel all remaining tasks
                    break;
                }
            }
        }
        _ => {}
    }

    // Launch next work item
    if let Some((rule, target)) = work_iter.next() {
        let checker = Arc::clone(&checker);
        join_set.spawn(async move {
            checker.check_file(&rule, &target).await
        });
    }
}
```

**Benefits:**
- ‚úÖ True cancellation - stops in-flight work
- ‚úÖ No wasted LLM calls
- ‚úÖ Full control over scheduling

**Tradeoffs:**
- ‚ùå More complex code (~80 lines)
- ‚ùå Returns Vec, not Stream (loses streaming behavior)
- ‚ùå Harder to compose with other stream operations

### Option 4: Hybrid - Early Stop with Buffer

```rust
let stream = stream::iter(work_queue)
    .scan(0, move |errors_found, (rule, target)| {
        // Check if we should continue
        if let Some(max) = max_errors {
            if *errors_found >= max {
                return None;  // Stop iteration
            }
        }

        Some((rule, target, *errors_found))
    })
    .map(move |(rule, target, _)| {
        let checker = Arc::clone(&checker);
        async move { checker.check_file(&rule, &target).await }
    })
    .buffer_unordered(concurrency);
```

**Problem:** `.scan()` can't see the results (they come out of buffer_unordered), so can't count errors.

## Recommended Approach

### For `concurrency = 4` and typical `max_errors = 1-5`

**Use Option 1 (Accept the Waste):**
- Worst case: 3 wasted calls when you hit max_errors
- Simple implementation
- Still get 4x speedup
- Not worth the complexity of cancellation

### For higher concurrency (e.g., 50)

**Use Option 2 (Cancellation Token):**
- Shared atomic counter
- Stop launching when limit reached
- In-flight checks detect flag and can skip work
- Worth the complexity to avoid 49 wasted calls

## Implementation for concurrency = 4

```rust
// Simple work queue with buffer_unordered
let work_items: Vec<(Rule, PathBuf)> = rules.iter()
    .flat_map(|rule| target_files.iter().map(move |file| (rule.clone(), file.clone())))
    .collect();

let checker = Arc::new(self.clone_for_streaming());
let concurrency = request.max_concurrency.unwrap_or(4);  // Default to 4

let stream = stream::iter(work_items)
    .map(move |(rule, target)| {
        let checker = Arc::clone(&checker);
        async move { checker.check_file(&rule, &target).await }
    })
    .buffer_unordered(concurrency)  // 4 concurrent checks
    .filter_map(|result| async move {
        match result {
            Ok(Some(violation)) if violation.severity == Severity::Error => Some(Ok(violation)),
            Ok(Some(_)) => None,
            Ok(None) => None,
            Err(e) => Some(Err(e)),
        }
    });

// Apply limits - this stops consuming but buffered futures complete
let limited_stream = if let Some(limit) = request.max_errors {
    stream.take(limit).boxed()
} else if request.check_mode == CheckMode::FailFast {
    stream.take(1).boxed()
} else {
    stream.boxed()
};
```

**With max_errors = 1:**
- Launches 4 checks in parallel
- First ERROR found ‚Üí stream yields it
- `.take(1)` stops consuming
- 3 other checks complete but results discarded
- **Waste:** 3 LLM calls
- **Speedup:** Still found first error 4x faster (race condition)

**With max_errors = 10:**
- Processes work queue 4 at a time
- After finding 10 errors, stops consuming
- **Waste:** Up to 3 LLM calls (the last buffered batch)
- **Speedup:** ~4x overall

## Why concurrency = 4 is Good

### 1. LLM Rate Limits
- Claude API: ~50 requests/minute for most tiers
- 4 concurrent = ~240 requests/hour
- Stays well under limits

### 2. Low Waste on Early Exit
- Max 3 wasted calls with max_errors
- Acceptable cost for 4x speedup

### 3. Resource Usage
- 4 concurrent checks √ó ~100KB = ~400KB memory
- Very reasonable

### 4. Cognitive Load
- Small enough to reason about
- Can trace 4 parallel operations mentally
- Easy to debug

## Answer to Your Questions

**Q: Limit parallel to 4?**
**A:** `max_concurrency: Some(4)` or default to 4

**Q: How to early exit with max_errors?**
**A:** `.take(max_errors)` on the stream stops consuming. With concurrency=4, you waste at most 3 LLM calls (buffered futures that complete). This is acceptable - not worth the complexity of cancellation.

**Alternative if waste is unacceptable:** Use cancellation token (Option 2 above) at cost of ~40 extra lines.

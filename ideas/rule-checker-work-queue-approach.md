# Rule Checker: Work Queue Approach

**THIS IS A SCRATCH FILE GENERATED BY A ðŸ¤–**

## Date
2025-11-13

## The Insight

Every (rule, file) combination is an **independent work item**. Instead of nested loops/streams, create a flat work queue and process it in parallel.

## Current Nested Approach (Complex)

```rust
stream::iter(rules)
    .flat_map(move |rule| {
        stream::iter(targets).then(move |target| {
            async move { checker.check_file(&rule, &target).await }
        })
    })
```

**Mental model:** For each rule, sequentially check all files, then next rule.

**Problems:**
- Nested structure is harder to reason about
- Sequential within each rule
- Can't easily parallelize

## Work Queue Approach (Simple)

```rust
// Step 1: Create all (rule, file) work items
let work_items: Vec<_> = rules.iter()
    .flat_map(|rule| {
        target_files.iter().map(move |file| (rule.clone(), file.clone()))
    })
    .collect();

// Step 2: Process work queue in parallel
let stream = stream::iter(work_items)
    .map(move |(rule, target)| {
        let checker = Arc::clone(&checker);
        async move { checker.check_file(&rule, &target).await }
    })
    .buffer_unordered(concurrency);
```

**Mental model:** Flat list of work. Process N items at a time.

**Benefits:**
- Simpler to understand
- Naturally parallelizes
- Easy to shuffle work order for better load balancing
- Can add work stealing, priority queues, etc.

## Example

**Input:**
- Rules: [no-unwrap, no-panic, function-length]
- Files: [main.rs, lib.rs]

**Work Queue:**
```
[
  (no-unwrap, main.rs),
  (no-unwrap, lib.rs),
  (no-panic, main.rs),
  (no-panic, lib.rs),
  (function-length, main.rs),
  (function-length, lib.rs),
]
```

**Processing with concurrency=3:**
- Batch 1: Check items 0, 1, 2 in parallel
- Batch 2: Check items 3, 4, 5 in parallel
- Done in 2 batches instead of 6 sequential steps

## Implementation

```rust
pub async fn check_streaming(
    &self,
    request: RuleCheckRequest,
) -> Result<impl Stream<Item = Result<RuleViolation>>> {
    // ... load rules, expand files, filter ...

    // Create flat work queue of (rule, file) pairs
    let work_items: Vec<(Rule, PathBuf)> = rules.iter()
        .flat_map(|rule| {
            target_files.iter().map(move |file| (rule.clone(), file.clone()))
        })
        .collect();

    tracing::info!("Created work queue with {} check items", work_items.len());

    // Process work queue in parallel
    let checker = Arc::new(self.clone_for_streaming());
    let concurrency = request.max_concurrency.unwrap_or(1); // Default sequential

    let stream = stream::iter(work_items)
        .map(move |(rule, target)| {
            let checker = Arc::clone(&checker);
            async move { checker.check_file(&rule, &target).await }
        })
        .buffer_unordered(concurrency)
        .filter_map(|result| async move {
            match result {
                Ok(Some(violation)) if violation.severity == Severity::Error => Some(Ok(violation)),
                Ok(Some(_)) => None,
                Ok(None) => None,
                Err(e) => Some(Err(e)),
            }
        });

    // Apply limits
    let limited_stream = if let Some(limit) = request.max_errors {
        stream.take(limit).boxed()
    } else if request.check_mode == CheckMode::FailFast {
        stream.take(1).boxed()
    } else {
        stream.boxed()
    };

    Ok(limited_stream)
}
```

## Advantages Over Nested Streams

### 1. Simpler Code
- One level of iteration, not two
- Flat structure easier to understand
- No nested closures and Arc cloning complexity

### 2. Better Observability
```rust
tracing::info!("Work queue: {} items", work_items.len());
tracing::info!("Concurrency: {}", concurrency);
tracing::info!("Estimated batches: {}", work_items.len() / concurrency);
```

Can report progress as work completes.

### 3. Work Reordering

Can prioritize work items:
```rust
// Check frequently-changed files first
work_items.sort_by_key(|(_, file)| {
    std::fs::metadata(file).ok()
        .and_then(|m| m.modified().ok())
        .map(|t| std::cmp::Reverse(t))
});
```

Or randomize for better load balancing:
```rust
use rand::seq::SliceRandom;
work_items.shuffle(&mut thread_rng());
```

### 4. Progress Tracking

```rust
let total_items = work_items.len();
let mut completed = 0;

let stream = stream::iter(work_items)
    .map(move |(rule, target)| {
        let checker = Arc::clone(&checker);
        async move {
            let result = checker.check_file(&rule, &target).await;
            // Send progress notification here
            result
        }
    })
    .buffer_unordered(concurrency);
```

Can send "Checked 50/1000 items" notifications.

### 5. Debugging

```rust
// Log each work item
for (i, (rule, file)) in work_items.iter().enumerate() {
    tracing::debug!("Work item {}: {} against {}", i, rule.name, file.display());
}
```

Easy to see exactly what will be checked and in what order.

## Performance Comparison

### Nested Sequential (Current)
```
Rule1: [File1 â†’ File2 â†’ File3]  (sequential)
Rule2: [File1 â†’ File2 â†’ File3]  (sequential)
```
Time: 6 Ã— 3s = 18s

### Work Queue Parallel
```
Batch1: [(R1,F1), (R1,F2), (R1,F3)]  (parallel)
Batch2: [(R2,F1), (R2,F2), (R2,F3)]  (parallel)
```
Time: 2 Ã— 3s = 6s with concurrency=3
Time: 1 Ã— 3s = 3s with concurrency=6

### Work Queue with Shuffling
```
Batch1: [(R1,F1), (R2,F2), (R1,F3)]  (parallel, mixed)
Batch2: [(R2,F1), (R1,F2), (R2,F3)]  (parallel, mixed)
```
Better load balancing if some rules are slower than others.

## Implementation Estimate

**Lines of code:** ~30
**Complexity:** Low (simpler than current nested approach)
**Risk:** Very low
**Benefit:** Up to Nx speedup where N = concurrency

## Configuration

### Conservative Default
```rust
max_concurrency: Some(10)  // Safe for most LLM rate limits
```

### Aggressive
```rust
max_concurrency: Some(50)  // Fast but may hit rate limits
```

### Sequential (Backward Compatible)
```rust
max_concurrency: None  // or Some(1)
```

## Fail-Fast Optimization

With work queue, can still do fail-fast efficiently:

```rust
if request.check_mode == CheckMode::FailFast {
    // Process work items in parallel but stop at first ERROR
    let stream = stream::iter(work_items)
        .map(|work_item| check_work_item(work_item))
        .buffer_unordered(concurrency)
        .filter_map(|result| async move {
            match result {
                Ok(Some(violation)) if violation.severity == Severity::Error => {
                    Some(Ok(violation))  // Return first ERROR and stream ends
                }
                _ => None
            }
        })
        .take(1);  // Stop after first ERROR
}
```

Even in fail-fast, parallelism helps: race to find the first error across N concurrent checks.

## Recommendation

Replace the nested `flat_map` + `then` with a flat work queue processed via `buffer_unordered`. This is:
- Simpler to understand
- Easier to debug
- Naturally parallel
- More flexible for future optimizations

The work queue approach is the standard pattern for embarrassingly parallel problems like this.

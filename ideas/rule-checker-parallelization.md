# Rule Checker Parallelization Analysis

**THIS IS A SCRATCH FILE GENERATED BY A ðŸ¤–**

## Date
2025-11-13

## Current Implementation

**Location**: `swissarmyhammer-rules/src/checker.rs:626-654`

```rust
let stream = stream::iter(rules)
    .flat_map(move |rule| {
        let targets = target_files.clone();
        let checker = Arc::clone(&checker);
        stream::iter(targets).then(move |target| {
            let rule = rule.clone();
            let checker = Arc::clone(&checker);
            async move { checker.check_file(&rule, &target).await }
        })
    })
    .filter_map(|result| async move {
        match result {
            Ok(Some(violation)) if violation.severity == Severity::Error => Some(Ok(violation)),
            Ok(Some(_)) => None, // Non-error violations logged but not yielded
            Ok(None) => None,    // PASS
            Err(e) => Some(Err(e)),
        }
    });
```

## Current Execution Pattern

**Sequential Processing:**
1. Take Rule 1
2. Check File 1 against Rule 1 (wait for LLM)
3. Check File 2 against Rule 1 (wait for LLM)
4. Check File 3 against Rule 1 (wait for LLM)
5. Take Rule 2
6. Check File 1 against Rule 2 (wait for LLM)
7. ... and so on

**Key observation:** `.then()` runs futures **sequentially** - each check_file call waits for the previous one to complete.

### Time Complexity (Sequential)
- N rules Ã— M files = N*M checks
- Each check takes ~1-5 seconds (LLM call)
- 10 rules Ã— 100 files = 1000 checks Ã— 3s = **3000 seconds = 50 minutes**

## Parallelization Opportunities

### Option 1: Buffered Unordered (Simple)

Replace `.then()` with `.buffer_unordered(concurrency_limit)`:

```rust
let stream = stream::iter(rules)
    .flat_map(move |rule| {
        let targets = target_files.clone();
        let checker = Arc::clone(&checker);
        stream::iter(targets).map(move |target| {
            let rule = rule.clone();
            let checker = Arc::clone(&checker);
            async move { checker.check_file(&rule, &target).await }
        })
        .buffer_unordered(10)  // Run 10 checks in parallel
    })
    .filter_map(|result| async move {
        // ... same filter logic ...
    });
```

**Benefits:**
- Simple change (`.then()` â†’ `.buffer_unordered(N)`)
- Controls concurrency (don't overwhelm LLM with 1000 concurrent requests)
- Maintains streaming (yields results as they complete)

**Performance:**
- 10 rules Ã— 100 files with concurrency=10
- ~100 batches of 10 parallel checks
- 100 batches Ã— 3s = **300 seconds = 5 minutes**
- **10x speedup**

**Tradeoffs:**
- Results arrive out of order (not deterministic)
- Need to manage LLM rate limits
- Higher memory usage (buffering futures)

### Option 2: Per-File Parallelism

Check all rules against one file in parallel:

```rust
let stream = stream::iter(target_files)
    .flat_map(move |target| {
        let rules_clone = rules.clone();
        let checker = Arc::clone(&checker);
        stream::iter(rules_clone).map(move |rule| {
            let target = target.clone();
            let checker = Arc::clone(&checker);
            async move { checker.check_file(&rule, &target).await }
        })
        .buffer_unordered(5)  // Check 5 rules per file in parallel
    })
    .filter_map(|result| async move {
        // ... filter logic ...
    });
```

**Benefits:**
- Groups work by file (better for cache locality)
- Can read file once, check multiple rules
- More intuitive ordering (all rules for file1, then file2, etc.)

**Tradeoffs:**
- Still sequential across files (file2 waits for file1)
- Doesn't help if you have 1 rule and 1000 files

### Option 3: Two-Level Parallelism (Best)

Parallel files AND parallel rules per file:

```rust
use futures::stream::StreamExt;

let stream = stream::iter(target_files)
    .map(move |target| {
        let rules_clone = rules.clone();
        let checker = Arc::clone(&checker);
        async move {
            // Check all rules for this file in parallel
            let checks = stream::iter(rules_clone)
                .map(move |rule| {
                    let target = target.clone();
                    let checker = Arc::clone(&checker);
                    async move { checker.check_file(&rule, &target).await }
                })
                .buffer_unordered(5);  // 5 rules per file in parallel

            // Collect results for this file
            checks.collect::<Vec<_>>().await
        }
    })
    .buffer_unordered(20)  // Process 20 files in parallel
    .flat_map(|file_results| stream::iter(file_results))
    .filter_map(|result| async move {
        // ... filter logic ...
    });
```

**Benefits:**
- Maximum parallelism: 20 files Ã— 5 rules = 100 concurrent checks
- Configurable at both levels
- Best overall throughput

**Performance:**
- 10 rules Ã— 100 files with file_concurrency=20, rule_concurrency=5
- Can run up to 100 parallel checks
- Limited by LLM rate limits, not execution model
- **Potentially 100x speedup** (limited by rate limits in practice)

### Option 4: Rayon (CPU Parallelism)

Use Rayon for CPU-bound parallelism:

```rust
use rayon::prelude::*;

let violations: Vec<_> = rules.par_iter()
    .flat_map(|rule| {
        target_files.par_iter().filter_map(move |target| {
            // Need to block on async - awkward
            tokio::task::block_in_place(|| {
                tokio::runtime::Handle::current()
                    .block_on(checker.check_file(rule, target))
            }).ok().flatten()
        })
    })
    .collect();
```

**Problems:**
- Rayon + async doesn't mix well (block_in_place is awkward)
- Loses streaming behavior (collects all results)
- Not idiomatic for async code

**Verdict:** Don't use Rayon, use async streams

## Constraints to Consider

### 1. LLM Rate Limits
- Claude API has rate limits (requests per minute)
- Too much parallelism = throttling/errors
- Need bounded concurrency (10-50 concurrent checks max)

### 2. Cache Effectiveness
- Cache is shared (Arc<RuleCache>)
- Thread-safe for concurrent access
- Parallel reads are fine
- Cache hits skip LLM entirely (fast path)

### 3. Fail-Fast Mode
- If `check_mode == FailFast`, stop after first ERROR
- `.take(1)` already handles this
- Parallelism still helps (race to find first error faster)

### 4. Memory Usage
- Each buffered future holds file content + rule + context
- 100 parallel checks Ã— ~100KB each = ~10MB
- Acceptable for most systems

### 5. Progress Notifications
- Currently not sent per-file (only at start/end)
- Parallelism makes progress tracking harder
- Could send notifications as files complete

## Recommendation

### Use Option 1: buffer_unordered (Simple)

**Start with simple parallelization:**
- Replace `.then()` with `.buffer_unordered(concurrency)`
- Add configuration parameter for concurrency limit
- Default to 10-20 concurrent checks

**Implementation:**
```rust
pub struct RuleCheckRequest {
    // ... existing fields ...

    /// Maximum number of concurrent rule checks (None = sequential)
    pub max_concurrency: Option<usize>,
}
```

Then:
```rust
let checks_stream = stream::iter(targets).map(move |target| {
    let rule = rule.clone();
    let checker = Arc::clone(&checker);
    async move { checker.check_file(&rule, &target).await }
});

// Apply parallelism if configured
let parallel_stream = if let Some(concurrency) = max_concurrency {
    checks_stream.buffer_unordered(concurrency).boxed()
} else {
    checks_stream.then(|f| f).boxed()  // Sequential
};
```

### Later: Upgrade to Option 3 (Two-Level)

Once simple parallelism is working:
- Add file-level concurrency
- Add rule-level concurrency
- Make both configurable
- Default: file_concurrency=20, rule_concurrency=5

## Testing Parallelism

### Verify Correctness
- Same violations found (parallel vs sequential)
- Cache still works correctly (concurrent access)
- Fail-fast still stops on first error
- No race conditions

### Measure Performance
- Time 100 files Ã— 10 rules (sequential vs parallel)
- Vary concurrency (1, 5, 10, 20, 50, 100)
- Find optimal concurrency for LLM rate limits
- Measure cache hit rate impact

### Edge Cases
- All cache hits (should be very fast in parallel)
- All cache misses (rate limiting becomes bottleneck)
- Mix of hits and misses
- One file, many rules
- Many files, one rule

## Compatibility

### Backward Compatible
- Add `max_concurrency: Option<usize>` with default `None` (sequential)
- Existing callers unchanged
- Opt-in to parallelism

### CLI Integration
```bash
# Sequential (default)
sah rule check src/**/*.rs

# Parallel
sah rule check src/**/*.rs --concurrency 20
```

### MCP Integration
```json
{
  "file_paths": ["src/**/*.rs"],
  "max_concurrency": 20
}
```

## Expected Impact

### Best Case (All Cache Hits)
- Sequential: ~1ms per cache lookup Ã— 1000 = 1 second
- Parallel(20): ~1ms Ã— 50 batches = 50ms
- **20x speedup**

### Worst Case (All Cache Misses)
- Sequential: ~3s per LLM call Ã— 1000 = 50 minutes
- Parallel(20): ~3s Ã— 50 batches = 2.5 minutes
- **20x speedup**

### Typical Case (50% cache hit rate)
- Sequential: (500 Ã— 1ms) + (500 Ã— 3s) = 25 minutes
- Parallel(20): (25 Ã— 1ms) + (25 Ã— 3s) = 75 seconds
- **20x speedup**

## Implementation Complexity

**Option 1 (buffer_unordered):** ~20 lines changed
**Option 3 (two-level):** ~50 lines changed

**Risk:** Low - futures_util::stream::buffer_unordered is well-tested

## Conclusion

**Start with Option 1**: Add `max_concurrency` parameter and use `.buffer_unordered()`. This gives massive speedup with minimal code changes and maintains streaming behavior.
